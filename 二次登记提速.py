import time
import traceback
import pandas as pd
import numpy as np
import threading
import os  # æ–°å¢ï¼šç”¨äºæ–‡ä»¶è·¯å¾„å¤„ç†
from concurrent.futures import ThreadPoolExecutor
from openpyxl import load_workbook
from openpyxl.styles import Alignment
from openpyxl.utils import get_column_letter
from pandas.io.excel import ExcelWriter


# --------------------------
# æ–°å¢ï¼šé«˜æ•ˆåˆå¹¶å•å…ƒæ ¼å‡½æ•°ï¼ˆåŸefficient_merge_cellsï¼‰
# --------------------------
def efficient_merge_cells(
        input_path,  # è¾“å…¥Excelè·¯å¾„ï¼ˆå³ä¿å­˜åçš„è¡¨2ï¼‰
        output_path,  # æœ€ç»ˆè¾“å‡ºè·¯å¾„ï¼ˆåˆå¹¶åçš„è¡¨2ï¼‰
        sheet_name,  # å·¥ä½œè¡¨åç§°
        group_col,  # åŸºå‡†åˆ—ï¼ˆå¦‚"è´¦å•æ‰¹æ¬¡"ï¼‰
        merge_cols,  # å¾…åˆå¹¶åˆ—ï¼ˆå¦‚["åº—é“ºä¸»ä½“"]ï¼‰
        header_row=1  # è¡¨å¤´è¡Œï¼ˆé»˜è®¤ç¬¬1è¡Œï¼‰
):
    """é«˜æ•ˆåˆå¹¶å•å…ƒæ ¼ï¼šå†…å­˜é¢„å¤„ç†+æ‰¹é‡åˆå¹¶ï¼Œ1ä¸‡è¡Œ<10ç§’"""
    start_merge_time = time.time()
    print(f"\nğŸ”— å¼€å§‹é«˜æ•ˆåˆå¹¶å•å…ƒæ ¼ï¼ˆåŸºå‡†åˆ—ï¼š{group_col}ï¼Œåˆå¹¶åˆ—ï¼š{merge_cols}ï¼‰")

    # 1. è¯»å–è¡¨2æ•°æ®ï¼ˆå†…å­˜é¢„å¤„ç†ï¼‰
    df = pd.read_excel(input_path, sheet_name=sheet_name, header=header_row - 1)
    total_rows = len(df)
    if total_rows == 0:
        print("âš ï¸ è¡¨2æ— æ•°æ®ï¼Œæ— éœ€åˆå¹¶")
        return

    # 2. æ•°æ®æ ¡éªŒ
    if group_col not in df.columns:
        raise ValueError(f"åŸºå‡†åˆ—'{group_col}'ä¸å­˜åœ¨äºè¡¨2ä¸­")
    for col in merge_cols:
        if col not in df.columns:
            raise ValueError(f"å¾…åˆå¹¶åˆ—'{col}'ä¸å­˜åœ¨äºè¡¨2ä¸­")

    # 3. å¡«å……ç©ºå€¼ï¼ˆé¿å…åˆå¹¶åˆ¤æ–­é”™è¯¯ï¼‰
    df[group_col] = df[group_col].fillna("__EMPTY__")

    # 4. è®¡ç®—åˆå¹¶èŒƒå›´ï¼ˆå†…å­˜æ“ä½œï¼Œå¿«ï¼‰
    merge_ranges = {col: [] for col in merge_cols}
    current_group = df[group_col].iloc[0]
    start_idx = 0

    for i in range(1, total_rows):
        if df[group_col].iloc[i] != current_group:
            # è½¬æ¢ä¸ºExcelè¡Œå·ï¼ˆè¡¨å¤´å 1è¡Œï¼Œæ•°æ®ä»header_row+1å¼€å§‹ï¼‰
            start_row = header_row + 1 + start_idx
            end_row = header_row + 1 + (i - 1)
            for col in merge_cols:
                merge_ranges[col].append((start_row, end_row))
            current_group = df[group_col].iloc[i]
            start_idx = i

    # å¤„ç†æœ€åä¸€ç»„
    start_row = header_row + 1 + start_idx
    end_row = header_row + 1 + (total_rows - 1)
    for col in merge_cols:
        merge_ranges[col].append((start_row, end_row))

    print(f"ğŸ“Š è®¡ç®—å®Œæˆï¼šå…±{len(merge_ranges[merge_cols[0]])}ç»„éœ€è¦åˆå¹¶")

    # 5. æ‰¹é‡åº”ç”¨åˆå¹¶ï¼ˆå‡å°‘Exceläº¤äº’ï¼‰
    wb = load_workbook(input_path)
    ws = wb[sheet_name]

    for col_name in merge_cols:
        col_idx = df.columns.get_loc(col_name) + 1  # è½¬ä¸ºExcel 1-basedåˆ—å·
        col_letter = get_column_letter(col_idx)
        merge_count = 0
        for (start_row, end_row) in merge_ranges[col_name]:
            # å®‰å…¨æ ¡éªŒï¼šé¿å…èŒƒå›´è¶Šç•Œ
            if start_row > end_row or end_row > ws.max_row:
                continue
            # æ‰§è¡Œåˆå¹¶
            ws.merge_cells(f"{col_letter}{start_row}:{col_letter}{end_row}")
            # å±…ä¸­å¯¹é½
            ws[f"{col_letter}{start_row}"].alignment = Alignment(horizontal='center', vertical='center')
            merge_count += 1
        print(f"âœ… åˆ—'{col_name}'åˆå¹¶å®Œæˆï¼š{merge_count}ç»„")

    # 6. å®‰å…¨ä¿å­˜
    try:
        wb.save(output_path)
        print(f"ğŸ’¾ åˆå¹¶åæ–‡ä»¶ä¿å­˜è‡³ï¼š{output_path}")
    except Exception as e:
        # ä¿å­˜å¤±è´¥æ—¶åˆ›å»ºå¤‡ä»½
        backup_path = output_path.replace(".xlsx", "_merge_backup.xlsx")
        wb.save(backup_path)
        print(f"âš ï¸ ä¸»æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼Œå·²å¤‡ä»½è‡³ï¼š{backup_path}ï¼Œé”™è¯¯ï¼š{str(e)}")
    wb.close()

    # è€—æ—¶ç»Ÿè®¡
    end_merge_time = time.time()
    print(f"â±ï¸  åˆå¹¶å•å…ƒæ ¼è€—æ—¶ï¼š{round(end_merge_time - start_merge_time, 2)}ç§’")


# --------------------------
# 1. åŸºç¡€å·¥å…·å‡½æ•°ï¼ˆä¸å˜ï¼‰
# --------------------------
def unmerge_and_fill(excel_path, sheet_name, save_path=None):
    wb = load_workbook(excel_path, data_only=True)
    ws = wb[sheet_name]
    merged_ranges = list(ws.merged_cells.ranges)

    # å…ˆè§£é™¤åˆå¹¶
    for merged_range in merged_ranges:
        ws.unmerge_cells(merged_range.coord)

    # å¡«å……åˆå¹¶åŒºåŸŸå€¼
    for merged_range in merged_ranges:
        min_col, min_row, max_col, max_row = merged_range.bounds
        main_value = ws.cell(row=min_row, column=min_col).value
        for row in range(min_row, max_row + 1):
            for col in range(min_col, max_col + 1):
                ws.cell(row=row, column=col, value=main_value)

    # è½¬ä¸ºDataFrameï¼ˆè¡¨å¤´åœ¨ç¬¬2è¡Œï¼Œæ•°æ®ä»ç¬¬3è¡Œå¼€å§‹ï¼‰
    headers = [cell.value for cell in ws[2]]
    data = [list(row) for row in ws.iter_rows(min_row=3, values_only=True)]
    df = pd.DataFrame(data, columns=headers, dtype=object)

    if save_path:
        wb.save(save_path)
    wb.close()
    return df


def select_shop():
    shops = [
        "æŠ–éŸ³-åä¸ºæ˜Ÿæ¡¥ä¸“å–åº—", "æŠ–éŸ³-vivoä¸½å¤ä¸“å–åº—", "æŠ–éŸ³-åä¸ºå´‡äº‘ä¸“å–åº—", "æŠ–éŸ³-åä¸ºæµ©æ˜Œæ•°ç ä¸“å–åº—",
        "äº¬ä¸œ-å´‡äº‘å¹³æ¿æ——èˆ°åº—", "æŠ–éŸ³-è£è€€æ˜Ÿæ¡¥ä¸“å–åº—", "æŠ–éŸ³-åä¸ºæ™ºæ…§é€šè¾¾ä¸“å–åº—", "æŠ–éŸ³-vivoå¹³æ¿æ——èˆ°åº—"
    ]
    print("=" * 70)
    print("ğŸ‰ æ¬¢è¿ä½¿ç”¨å›½è¡¥äºŒæ¬¡ç™»è®°ç³»ç»Ÿï¼ˆV2.1 - é«˜æ•ˆåˆå¹¶ç‰ˆï¼‰")
    print("âš ï¸  è¯·ç¡®ä¿å·²å®Œæˆï¼š1. å›½è¡¥è¡¨å­˜æ ¹ç›®å½• 2. å«èµ„æ¬¾æ–‡ä»¶åœ¨ã€Œä¸­é—´æ–‡ä»¶â€”å¯å¿½ç•¥ã€")
    print("=" * 70)
    print("ğŸª è¯·é€‰æ‹©å¤„ç†çš„åº—é“ºï¼ˆè¾“å…¥1-8ï¼‰ï¼š")
    print("-" * 70)
    for i, shop in enumerate(shops, 1):
        print(f"   {i:2d} â†’ {shop}")
    print("-" * 70)

    while True:
        user_input = input("è¯·è¾“å…¥é€‰æ‹©ï¼ˆ1-8ï¼‰ï¼š").strip()
        if not user_input.isdigit():
            print(f"âŒ è¾“å…¥é”™è¯¯ï¼è¯·è¾“1-8ï¼ˆå½“å‰ï¼š{user_input}ï¼‰")
            continue
        select_num = int(user_input)
        if 1 <= select_num <= 8:
            selected_shop = shops[select_num - 1]
            print(f"\nâœ… å·²é€‰æ‹©åº—é“ºï¼š{selected_shop}")
            print("=" * 70)
            return selected_shop
        else:
            print(f"âŒ è¶…å‡ºèŒƒå›´ï¼è¯·è¾“1-8ï¼ˆå½“å‰ï¼š{select_num}ï¼‰")


# --------------------------
# 2. å¤šçº¿ç¨‹æ ¸å¿ƒï¼šå¹¶è¡Œå¤„ç†è¡¨1çš„SKUåŒ¹é…ï¼ˆä¸å˜ï¼‰
# --------------------------
def process_table1_batch(batch_idx, batch_data, df2_grouped, sku_counts, df1, lock):
    print(f"ğŸ“Œ çº¿ç¨‹{batch_idx}å¼€å§‹å¤„ç†ï¼Œæ‰¹æ¬¡æ•°æ®é‡ï¼š{len(batch_data)}è¡Œ")
    start_time = time.time()

    # å‘é‡åŒ–å‡½æ•°
    def clean_cost(cost):
        try:
            return float(str(cost).replace('Â¥', '').replace(' ', '').strip())
        except (ValueError, TypeError):
            return np.nan

    def clean_order_amt(amt):
        try:
            return float(str(amt).replace('Â¥', '').replace(' ', '').strip())
        except (ValueError, TypeError):
            return 0

    # å¤„ç†æ‰¹æ¬¡æ•°æ®
    for idx, row in batch_data:
        current_sku = row["skuå•å·"]
        cost_num = clean_cost(row["é‡‡è´­æˆæœ¬ï¼ˆå…ƒï¼‰"])
        count = sku_counts.get(current_sku, 0)
        status = ""

        # åŒ¹é…é€»è¾‘
        if pd.isna(cost_num):
            status = "æœªåŒ¹é…_é‡‡è´­æˆæœ¬æ ¼å¼é”™è¯¯"
        elif count == 0:
            status = "æœªåŒ¹é…_æœªæ‰¾åˆ°åŒ¹é…"
        elif count == 2:
            status = "ä¸¤ä¸ªå•å·"
            try:
                matching_rows = df2_grouped.get_group(current_sku).copy()
                matching_rows["è®¢å•é‡‘é¢_æ•°å€¼"] = matching_rows["è®¢å•é‡‘é¢"].apply(clean_order_amt)
                filtered = matching_rows[matching_rows["è®¢å•é‡‘é¢_æ•°å€¼"] > 0] if cost_num > 0 else matching_rows[matching_rows["è®¢å•é‡‘é¢_æ•°å€¼"] < 0]
                if not filtered.empty:
                    first_idx = filtered.index[0]
                    with lock:
                        fill_cols = [
                            ("è´¦å•æ‰¹æ¬¡â€”1", "è´¦å•æ‰¹æ¬¡"), ("è¡Œç±»å‹â€”1", "è¡Œç±»å‹"), ("è®¢å•åº”ä»˜é‡‘é¢ï¼ˆå…ƒï¼‰â€”1", "è®¢å•åº”ä»˜é‡‘é¢ï¼ˆå…ƒï¼‰"),
                            ("æ”¿åºœè¡¥è´´ï¼ˆå…ƒï¼‰â€”1", "æ”¿åºœè¡¥è´´ï¼ˆå…ƒï¼‰"), ("åº—é“ºè¡¥è´´ï¼ˆå…ƒï¼‰â€”1", "åº—é“ºè¡¥è´´ï¼ˆå…ƒï¼‰"), ("è‡ªè¥è¡¥è´´ï¼ˆå…ƒï¼‰â€”1", "è‡ªè¥è¡¥è´´ï¼ˆå…ƒï¼‰"),
                            ("åˆ†è´¦é‡‘é¢ï¼ˆå…ƒï¼‰â€”1", "åˆ†è´¦é‡‘é¢ï¼ˆå…ƒï¼‰"), ("æœåŠ¡è´¹ç”¨ï¼ˆå…ƒï¼‰â€”1", "æœåŠ¡è´¹ç”¨ï¼ˆå…ƒï¼‰"), ("å¹³å°æŠ˜æ‰£ï¼ˆå…ƒï¼‰â€”1", "å¹³å°æŠ˜æ‰£ï¼ˆå…ƒï¼‰"),
                            ("è®¢å•å®ä»˜ï¼ˆå…ƒï¼‰â€”1", "è®¢å•å®ä»˜ï¼ˆå…ƒï¼‰"), ("é‡‡è´­æŠ˜æ‰£æ¯”ä¾‹â€”1", "é‡‡è´­æŠ˜æ‰£æ¯”ä¾‹"), ("é‡‡è´­æŠ˜æ‰£é‡‘é¢ï¼ˆå…ƒï¼‰â€”1", "é‡‡è´­æŠ˜æ‰£é‡‘é¢ï¼ˆå…ƒï¼‰"),
                            ("é‡‡è´­æˆæœ¬ï¼ˆå…ƒï¼‰â€”1", "é‡‡è´­æˆæœ¬ï¼ˆå…ƒï¼‰"), ("ç»“ç®—é‡‘é¢ï¼ˆå…ƒï¼‰â€”1", "ç»“ç®—é‡‘é¢ï¼ˆå…ƒï¼‰"), ("åˆ›å»ºæ—¶é—´â€”1", "åˆ›å»ºæ—¶é—´"), ("å¤‡æ³¨â€”1", "å¤‡æ³¨")
                        ]
                        for target, source in fill_cols:
                            if source in row.index:
                                df2.at[first_idx, target] = row[source]
                else:
                    status = "æœªåŒ¹é…_æ— å¯¹åº”æ­£è´Ÿè®¢å•é‡‘é¢"
            except KeyError:
                status = "æœªåŒ¹é…_æ— å¯¹åº”è®¢å•"
        elif count > 2:
            status = "æœªåŒ¹é…_åŒ¹é…è¿‡å¤šï¼Œæ— æ³•æ’é™¤"
        else:
            status = "æ­£å¸¸åŒ¹é…"
            try:
                matching_rows = df2_grouped.get_group(current_sku)
                first_idx = matching_rows.index[0]
                with lock:
                    fill_cols = [
                        ("è´¦å•æ‰¹æ¬¡â€”1", "è´¦å•æ‰¹æ¬¡"), ("è¡Œç±»å‹â€”1", "è¡Œç±»å‹"), ("è®¢å•åº”ä»˜é‡‘é¢ï¼ˆå…ƒï¼‰â€”1", "è®¢å•åº”ä»˜é‡‘é¢ï¼ˆå…ƒï¼‰"),
                        ("æ”¿åºœè¡¥è´´ï¼ˆå…ƒï¼‰â€”1", "æ”¿åºœè¡¥è´´ï¼ˆå…ƒï¼‰"), ("åº—é“ºè¡¥è´´ï¼ˆå…ƒï¼‰â€”1", "åº—é“ºè¡¥è´´ï¼ˆå…ƒï¼‰"), ("è‡ªè¥è¡¥è´´ï¼ˆå…ƒï¼‰â€”1", "è‡ªè¥è¡¥è´´ï¼ˆå…ƒï¼‰"),
                        ("åˆ†è´¦é‡‘é¢ï¼ˆå…ƒï¼‰â€”1", "åˆ†è´¦é‡‘é¢ï¼ˆå…ƒï¼‰"), ("æœåŠ¡è´¹ç”¨ï¼ˆå…ƒï¼‰â€”1", "æœåŠ¡è´¹ç”¨ï¼ˆå…ƒï¼‰"), ("å¹³å°æŠ˜æ‰£ï¼ˆå…ƒï¼‰â€”1", "å¹³å°æŠ˜æ‰£ï¼ˆå…ƒï¼‰"),
                        ("è®¢å•å®ä»˜ï¼ˆå…ƒï¼‰â€”1", "è®¢å•å®ä»˜ï¼ˆå…ƒï¼‰"), ("é‡‡è´­æŠ˜æ‰£æ¯”ä¾‹â€”1", "é‡‡è´­æŠ˜æ‰£æ¯”ä¾‹"), ("é‡‡è´­æŠ˜æ‰£é‡‘é¢ï¼ˆå…ƒï¼‰â€”1", "é‡‡è´­æŠ˜æ‰£é‡‘é¢ï¼ˆå…ƒï¼‰"),
                        ("é‡‡è´­æˆæœ¬ï¼ˆå…ƒï¼‰â€”1", "é‡‡è´­æˆæœ¬ï¼ˆå…ƒï¼‰"), ("ç»“ç®—é‡‘é¢ï¼ˆå…ƒï¼‰â€”1", "ç»“ç®—é‡‘é¢ï¼ˆå…ƒï¼‰"), ("åˆ›å»ºæ—¶é—´â€”1", "åˆ›å»ºæ—¶é—´"), ("å¤‡æ³¨â€”1", "å¤‡æ³¨")
                    ]
                    for target, source in fill_cols:
                        if source in row.index:
                            df2.at[first_idx, target] = row[source]
            except KeyError:
                status = "æœªåŒ¹é…_æ— å¯¹åº”è®¢å•"

        # åŠ é”æ›´æ–°çŠ¶æ€
        with lock:
            df1.at[idx, "äºŒæ¬¡ç™»è®°çŠ¶æ€"] = status

    end_time = time.time()
    print(f"âœ… çº¿ç¨‹{batch_idx}å¤„ç†å®Œæˆï¼Œè€—æ—¶ï¼š{round(end_time - start_time, 2)}ç§’")


# --------------------------
# 3. ä¸»å¤„ç†å‡½æ•°ï¼ˆæ•´åˆé«˜æ•ˆåˆå¹¶ï¼‰
# --------------------------
def process_excel_files(table1_path, table2_path, output_table1_path, output_table2_path, sheet_name, max_threads=4):
    global df2
    lock = threading.Lock()
    start_total_time = time.time()

    # --------------------------
    # å¹¶è¡Œæ­¥éª¤1ï¼šè¯»å–è¡¨1å’Œè¡¨2
    # --------------------------
    print("ğŸ” å¼€å§‹å¹¶è¡Œè¯»å–åŸå§‹æ–‡ä»¶...")
    with ThreadPoolExecutor(max_workers=2) as executor:
        future_table1 = executor.submit(pd.read_excel, table1_path, dtype=object)
        future_table2 = executor.submit(unmerge_and_fill, table2_path, sheet_name)
        df1 = future_table1.result()
        df2 = future_table2.result()
    print(f"âœ… è¡¨1ï¼ˆ{len(df1)}è¡Œï¼‰+ è¡¨2ï¼ˆ{len(df2)}è¡Œï¼‰è¯»å–å®Œæˆ")

    # --------------------------
    # æ•°æ®æ ¡éªŒ
    # --------------------------
    required_fields1 = ["skuå•å·", "è´¦å•æ‰¹æ¬¡", "é‡‡è´­æˆæœ¬ï¼ˆå…ƒï¼‰", "æœåŠ¡è´¹ç”¨ï¼ˆå…ƒï¼‰"]
    required_fields2 = ["skuå•å·", "è´¦å•æ‰¹æ¬¡â€”1"]
    missing1 = [f for f in required_fields1 if f not in df1.columns]
    missing2 = [f for f in required_fields2 if f not in df2.columns]
    if missing1 or missing2:
        error_msg = []
        if missing1:
            error_msg.append(f"è¡¨1ç¼ºå°‘å­—æ®µ: {', '.join(missing1)}")
        if missing2:
            error_msg.append(f"è¡¨2ç¼ºå°‘å­—æ®µ: {', '.join(missing2)}")
        raise ValueError("; ".join(error_msg))
    print("âœ… å­—æ®µæ ¡éªŒé€šè¿‡")

    # --------------------------
    # è¡¨1é¢„å¤„ç†
    # --------------------------
    df1["äºŒæ¬¡ç™»è®°çŠ¶æ€"] = ""
    df2_grouped = df2.groupby("skuå•å·")
    sku_counts = df2["skuå•å·"].value_counts()
    print(f"ğŸ“Š è¡¨2 SKUç»Ÿè®¡ï¼š{len(sku_counts)}ä¸ªä¸åŒSKUï¼Œæœ€å¤šé‡å¤{sku_counts.max()}æ¬¡")

    # --------------------------
    # å¹¶è¡Œæ­¥éª¤2ï¼šå¤šçº¿ç¨‹å¤„ç†è¡¨1 SKUåŒ¹é…
    # --------------------------
    print(f"\nâš™ï¸  å¤šçº¿ç¨‹å¤„ç†è¡¨1 SKUåŒ¹é…ï¼ˆçº¿ç¨‹æ•°ï¼š{max_threads}ï¼Œæ€»è¡Œæ•°ï¼š{len(df1)}ï¼‰")
    total_rows = len(df1)
    batch_size = total_rows // max_threads if total_rows >= max_threads else total_rows
    batches = []
    for i in range(max_threads):
        start_idx = i * batch_size
        end_idx = (i + 1) * batch_size if i < max_threads - 1 else total_rows
        if start_idx >= end_idx:
            break
        batch_data = list(df1.iloc[start_idx:end_idx].iterrows())
        batches.append((i + 1, batch_data))

    # æ‰§è¡Œå¤šçº¿ç¨‹
    with ThreadPoolExecutor(max_workers=len(batches)) as executor:
        futures = []
        for batch_idx, batch_data in batches:
            future = executor.submit(
                process_table1_batch,
                batch_idx=batch_idx,
                batch_data=batch_data,
                df2_grouped=df2_grouped,
                sku_counts=sku_counts,
                df1=df1,
                lock=lock
            )
            futures.append(future)
        for future in futures:
            future.result()
    print(f"âœ… è¡¨1 SKUåŒ¹é…å®Œæˆ")

    # --------------------------
    # ä¸²è¡Œæ­¥éª¤3ï¼šä¿å­˜è¡¨1 + è¡¨2ï¼ˆå«SKUæ ¼å¼è®¾ç½®ï¼‰
    # --------------------------
    print(f"\nğŸ’¾ å¼€å§‹ä¿å­˜åŸºç¡€ç»“æœæ–‡ä»¶...")
    start_save_time = time.time()

    # ä¿å­˜è¡¨1
    df1.to_excel(output_table1_path, index=False, engine='openpyxl')
    print(f"âœ… è¡¨1ä¿å­˜è‡³ï¼š{output_table1_path}ï¼ˆ{round(os.path.getsize(output_table1_path)/1024, 1)} KBï¼‰")

    # ä¿å­˜è¡¨2ï¼ˆå«SKUæ–‡æœ¬æ ¼å¼ï¼‰
    with ExcelWriter(output_table2_path, engine='openpyxl') as writer:
        df2.to_excel(writer, sheet_name="Sheet1", index=False)
        wb = writer.book
        ws = writer.sheets["Sheet1"]

        # æ‰¹é‡è®¾ç½®SKUåˆ—æ ¼å¼ï¼ˆæ•´åˆ—ï¼‰
        try:
            sku_col_idx = df2.columns.get_loc("skuå•å·") + 1
            sku_col_letter = get_column_letter(sku_col_idx)
            ws.column_dimensions[sku_col_letter].number_format = '@'
            print(f"âœ… SKUåˆ—ï¼ˆ{sku_col_letter}åˆ—ï¼‰è®¾ä¸ºæ–‡æœ¬æ ¼å¼")
        except ValueError:
            print(f"âš ï¸ æœªæ‰¾åˆ°'skuå•å·'åˆ—ï¼Œè·³è¿‡æ ¼å¼è®¾ç½®")
    print(f"âœ… è¡¨2åŸºç¡€ç‰ˆä¿å­˜è‡³ï¼š{output_table2_path}ï¼ˆ{round(os.path.getsize(output_table2_path)/1024, 1)} KBï¼‰")

    # --------------------------
    # æ–°å¢æ­¥éª¤4ï¼šè°ƒç”¨é«˜æ•ˆåˆå¹¶å‡½æ•°å¤„ç†è¡¨2åˆå¹¶
    # --------------------------
    # å®šä¹‰åˆå¹¶å‚æ•°ï¼ˆåŸºå‡†åˆ—ï¼šè´¦å•æ‰¹æ¬¡ï¼Œåˆå¹¶åˆ—ï¼šåº—é“ºä¸»ä½“ï¼‰
    merge_group_col = "è´¦å•æ‰¹æ¬¡"
    merge_target_cols = ["åº—é“ºä¸»ä½“"]
    # è°ƒç”¨é«˜æ•ˆåˆå¹¶ï¼ˆè¾“å…¥ï¼šåŸºç¡€ç‰ˆè¡¨2ï¼›è¾“å‡ºï¼šåˆå¹¶åçš„è¡¨2ï¼‰
    efficient_merge_cells(
        input_path=output_table2_path,  # åˆšä¿å­˜çš„è¡¨2åŸºç¡€ç‰ˆ
        output_path="å›½è¡¥_å·²åˆå¹¶.xlsx",  # ç›´æ¥è¦†ç›–æˆ–æ”¹ä¸ºæ–°è·¯å¾„ï¼ˆå¦‚"å›½è¡¥_å·²åˆå¹¶.xlsx"ï¼‰
        sheet_name="Sheet1",
        group_col=merge_group_col,
        merge_cols=merge_target_cols,
        header_row=1
    )

    # --------------------------
    # æœ€ç»ˆç»Ÿè®¡
    # --------------------------
    end_total_time = time.time()
    print("\n" + "=" * 70)
    print("ğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š å¤„ç†æ€»ç»“ï¼š")
    print(f"   â€¢ æ€»è€—æ—¶ï¼š{round(end_total_time - start_total_time, 2)}ç§’")
    print(f"   â€¢ è¡¨1ï¼š{len(df1)}è¡Œ â†’ {output_table1_path}")
    print(f"   â€¢ è¡¨2ï¼š{len(df2)}è¡Œï¼ˆå«{len(merge_target_cols)}åˆ—åˆå¹¶ï¼‰â†’ {output_table2_path}")
    print(f"   â€¢ åŒ¹é…çŠ¶æ€ï¼š{df1['äºŒæ¬¡ç™»è®°çŠ¶æ€'].value_counts().to_dict()}")
    print("=" * 70)


# --------------------------
# ä¸»å‡½æ•°è°ƒç”¨
# --------------------------
if __name__ == "__main__":
    # æ–‡ä»¶è·¯å¾„é…ç½®
    table1_path = "./ä¸­é—´æ–‡ä»¶â€”å¯å¿½ç•¥/å«èµ„æ¬¾ç»“æœ_æœªå¤„ç†.xlsx"
    table2_path = "å›½è¡¥è¡¨.xlsx"
    output_table1_path = "å«èµ„æ¬¾_å·²æ ‡è®°.xlsx"
    output_table2_path = "å›½è¡¥_å·²æ›´æ–°.xlsx"  # åˆå¹¶åä¼šè¦†ç›–æ­¤æ–‡ä»¶ï¼ˆæˆ–æ”¹ä¸ºæ–°è·¯å¾„ï¼‰

    try:
        # 1. é€‰æ‹©åº—é“º
        sheet_name = select_shop()

        # 2. è‡ªåŠ¨è·å–CPUæ ¸å¿ƒæ•°è®¾ç½®çº¿ç¨‹æ•°
        max_threads = os.cpu_count() or 4
        print(f"âš™ï¸  ç³»ç»Ÿæ£€æµ‹åˆ°{os.cpu_count()}ä¸ªCPUæ ¸å¿ƒï¼Œä½¿ç”¨{max_threads}ä¸ªçº¿ç¨‹")

        # 3. æ‰§è¡Œå¤„ç†
        process_excel_files(
            table1_path=table1_path,
            table2_path=table2_path,
            output_table1_path=output_table1_path,
            output_table2_path=output_table2_path,
            sheet_name=sheet_name,
            max_threads=max_threads
        )
    except Exception as e:
        print(f"\nâŒ æ“ä½œå¤±è´¥: {str(e)}")
        traceback.print_exc()